<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-03T15:25:32-03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">tuts blog</title><subtitle>Hi, I&apos;m Artur! I&apos;m currently a Data Scientist at Amazon, working here in São Paulo. I usually don&apos;t post that much, only when I&apos;m feeling really inspired. Feel free to contact me if you have any questions.</subtitle><entry><title type="html">Blog revamp</title><link href="http://localhost:4000/2024/03/02/primeiro_post.html" rel="alternate" type="text/html" title="Blog revamp" /><published>2024-03-02T18:45:54-03:00</published><updated>2024-03-02T18:45:54-03:00</updated><id>http://localhost:4000/2024/03/02/primeiro_post</id><content type="html" xml:base="http://localhost:4000/2024/03/02/primeiro_post.html"><![CDATA[<p>Hi there, I’m revamping my old blog to this new one.</p>

<p>Previously, I was using <a href="https://getpelican.com/">pelican</a> with Travis CI to deploy it. Travis had a couple of changes over the years I’m now I too bored to
work on it. So I decided to try out Jekyll, after reading <a href="https://karpathy.github.io/">Karpathy’s blog</a>.</p>

<p>I am adding the old post I still find useful.</p>

<p>That’s all folks!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Hi there, I’m revamping my old blog to this new one.]]></summary></entry><entry><title type="html">Não linearidade em redes neurais com Pytorch</title><link href="http://localhost:4000/2020/11/14/non-linear.html" rel="alternate" type="text/html" title="Não linearidade em redes neurais com Pytorch" /><published>2020-11-14T15:16:54-03:00</published><updated>2020-11-14T15:16:54-03:00</updated><id>http://localhost:4000/2020/11/14/non-linear</id><content type="html" xml:base="http://localhost:4000/2020/11/14/non-linear.html"><![CDATA[<p>Dentro do contexto de redes neurais, as funções de ativação são essenciais. Uma de suas principais funções é permitir o aprendizado de regiões e superfícies de decisão mais complexas. Nesse post, iremos criar redes neurais usando <code class="language-plaintext highlighter-rouge">Pytorch</code> e analisar as superfícies de decisão com e sem funções de ativação.</p>

<h2 id="redes-neurais">Redes neurais</h2>

<p>Uma rede neural é um aproximador universal de funções. O que isso quer dizer? Basicamente, uma rede neural consegue aproximar qualquer função contínua (<a href="http://neuralnetworksanddeeplearning.com/chap4.html">dadas algumas condições</a>). Quando estamos tratando de aprendizado de máquina, de modo geral, a ideia é essencialmente aproximar uma função que descreve/aprende seus dados de treino.</p>

<p>Dessa forma, redes neurais são ferramentas muito poderosas em problemas de classificação, em que procuramos uma superfície de decisão para conseguir dizer a qual classe um objeto pertence, e regressão, em que predizemos um valor numérico. Nesse post, iremos abordar o uso das redes em problemas de classificação.</p>

<p>Algumas peças importantes compõem a arquitetura de uma rede neural, e os componentes básicos são: os neurônios (unidades básicas), a entrada (<em>inputs</em>), as camadas internas (<em>hidden layers</em>), as funções de ativação (<em>activation functions</em>) e a camada de saída (<em>outputs</em>). Na imagem abaixo, temos uma ilustração de uma rede:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/simple_nn.png" alt="neuralnet" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Rede neural com uma camada de entrada, uma camada interna</td>
    </tr>
    <tr>
      <td style="text-align: center">e uma camada de saída. Fonte: <a href="https://commons.wikimedia.org/wiki/File:Neuralnetwork.png">Wikipedia</a></td>
    </tr>
  </tbody>
</table>

<p>Não entraremos em detalhes sobre outros pontos igualmente relevantes no estudo de redes neurais, como funções de perda (<em>loss functions</em>), otimizadores (<em>optimizers</em>), <em>backpropagation</em>, etc. Em especial, iremos analisar o impacto das funções de ativação quando temos um problema de classificação com uma superfície não linear.</p>

<h3 id="exemplo-em-pytorch">Exemplo em Pytorch</h3>

<p>Um neurônio, unidade fundamental de uma rede neural, computa uma função linear do que recebe de entrada:
\(y = w*x + b\)</p>

<p>Como podemos perceber, essa equação define uma reta. Em <code class="language-plaintext highlighter-rouge">Pytorch</code>, podemos descrever essa função como:</p>

<script src="https://gist.github.com/tutss/eea6c62bd60a5d146613b982e51ab50d.js"></script>

<!-- ```python
import torch.nn as nn
model = nn.Linear(1,1)
y = model(x)
``` -->

<p>Se os neurônios definem retas, como uma rede pode aproximar tão bem funções complexas? Afinal, combinações de funções lineares são lineares. É ai que entra a importância das funções de ativação.</p>

<h2 id="funções-de-ativação">Funções de ativação</h2>

<p>Como comentado anteriormente, um dos principais papéis das funções de ativação é adicionar não linearidade na saída de um neurônio, permitindo que a rede consiga aprender superfícies de decisão mais complexas. Existem diferentes tipos de funções de ativação, veja na figura abaixo:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/activ_func.png" alt="activationfunc" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Exemplo de duas funções de ativação: ReLU e <em>Softplus</em>. Fonte: <a href="https://commons.wikimedia.org/wiki/File:Rectified_linear_and_Softplus_activation_functions.png">Wikipedia</a></td>
    </tr>
  </tbody>
</table>

<p>Um exemplo comum de função de ativação é a função sigmóide, descrita matematicamente como:
\(f(x) = \frac{1}{1+e^{-x}}\)
A função sigmóide é não linear e o seu resultado são valores no intervalo entre 0 e 1. Ela, como as demais funções de ativação, introduzem o fator não linearidade na rede quando um neurônio computa:
\(f(y) = f(w*x + b) = \frac{1}{1+e^{-(wx+b)}}\)</p>

<p>No nosso caso, o problema de classificação em questão é obtido pela função <code class="language-plaintext highlighter-rouge">make_moons</code> do pacote <code class="language-plaintext highlighter-rouge">scikit-learn</code>. Nele, temos duas features que descrevem duas classes, como na figura abaixo.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/makemoons.png" alt="makemoons" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Dataset <em>make_moons</em>.</td>
    </tr>
  </tbody>
</table>

<p>Iremos inicialmente montar uma pequena rede, sem nenhuma função de ativação, e verificar a superfície de decisão que ela obtêm.</p>

<h2 id="avaliando-as-redes-no-conjunto-make_moons">Avaliando as redes no conjunto <em>make_moons</em></h2>

<p>O pacote <a href="pytorch.org/">Pytorch</a> possui alguns módulos que auxiliam e simplificam a construção de uma rede neural. No nosso caso, iremos utilizar o módulo <code class="language-plaintext highlighter-rouge">nn</code>, que contém métodos e classes necessárias para a construção da rede, e o módulo <code class="language-plaintext highlighter-rouge">optim</code>, que contém os otimizadores para as redes.</p>

<p>Primeiro, é importante lembrar que o Pytorch trabalha com uma estrutura de dados chamada <strong>tensor</strong>. Um tensor seria uma generalização de um vetor em $N$ dimensões. Por exemplo, um vetor é um tensor com 1 dimensão e uma matriz é um tensor com 2 dimensões. O pacote <code class="language-plaintext highlighter-rouge">scikit-learn</code>, que contém a função para construirmos o conjunto de dados, utiliza vetores do <code class="language-plaintext highlighter-rouge">numpy</code>, e portanto, precisamos convertê-los para a estrutura <code class="language-plaintext highlighter-rouge">torch.tensor</code>.</p>

<h3 id="convertendo-para-tensor">Convertendo para tensor</h3>

<p>Obtemos o conjunto de dados:</p>

<script src="https://gist.github.com/tutss/45d7acb84bac1dde4b7fa7e5ed4321b4.js"></script>

<!-- ```python
n_samples = 1_000
X, y = make_moons(n_samples=n_samples, noise=0.1, random_state=42)
``` -->

<p>Dividimos o conjunto em treino e teste:</p>

<script src="https://gist.github.com/tutss/0c8299a296ac50a84b6d564921066e39.js"></script>

<!-- ```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)
``` -->

<p>Convertendo para tensor:</p>

<script src="https://gist.github.com/tutss/b4fbff0722d824d422bfdb9516883c2d.js"></script>

<!-- ```python
X_train_tensor = torch.from_numpy(X_train)
y_train_tensor = torch.from_numpy(y_train).unsqueeze(1)

<!-- X_val_tensor = torch.from_numpy(X_val)
y_val_tensor = torch.from_numpy(y_val).unsqueeze(1)
``` -->

<h3 id="módulos-nn-e-optim">Módulos <code class="language-plaintext highlighter-rouge">nn</code> e <code class="language-plaintext highlighter-rouge">optim</code></h3>

<p>O módulo <code class="language-plaintext highlighter-rouge">nn</code> contém componentes importantes na construção de uma rede, como o tipo das camadas, as funções de perda e ativação que iremos utilizar. Já o módulo <code class="language-plaintext highlighter-rouge">optim</code> contém os otimizadores para a rede.</p>

<script src="https://gist.github.com/tutss/3be84f7f761d67ec251d8465c86a4b46.js"></script>

<!-- ```python
import torch.nn as nn
import torch.optim as optim
``` -->

<hr />

<h2 id="construindo-as-redes">Construindo as redes</h2>

<p>Agora temos as peças necessárias para construir as redes que irão classificar o nosso conjunto. Assim como no pacote Keras, o Pytorch possui um módulo chamado <a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"><code class="language-plaintext highlighter-rouge">nn.Sequential</code></a> que facilita bastante a construção da rede. Também, precisamos de um otimizador, no nosso caso, <a href="https://pytorch.org/docs/stable/_modules/torch/optim/adagrad.html#Adagrad"><code class="language-plaintext highlighter-rouge">Adagrad</code></a> e a função de perda <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss"><code class="language-plaintext highlighter-rouge">nn.BCELoss</code></a>.</p>

<p>Aqui vale ressaltar que a escolha do otimizador e da função de perda não tiveram uma grande motivação, apenas que o <code class="language-plaintext highlighter-rouge">Adagrad</code> é um otimizador comum, e a função de perda é apropriada para classificação binária (<a href="https://en.wikipedia.org/wiki/Cross_entropy"><em>binary cross entropy</em></a>).</p>

<h3 id="1-modelo">1° modelo</h3>

<p>Para o primeiro modelo, com 100 unidades em uma única camada interna, temos:</p>

<script src="https://gist.github.com/tutss/744c13a2092b093b7b7dd948abcb16b4.js"></script>

<!-- ```python
model = nn.Sequential(
    nn.Linear(input_shape, 100),
    nn.Linear(100, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(model.parameters())
loss = nn.BCELoss()
``` -->

<p>Com a rede construída, precisamos treiná-la para que possa aprender as características dos dados e realizar as previsões. A função de treino:</p>

<script src="https://gist.github.com/tutss/6e55bb5f4ea271db57dc36da338aa262.js"></script>

<!-- ```python
def train(model, loss, optimizer, train_values, train_target, val_values, val_target, epochs=50):
    for e in range(1, epochs+1):
        optimizer.zero_grad()   
        output = model(train_values.float())
        loss_train = loss(output, train_target.float())
        
<!-- val_output = model(val_values.float())
        loss_val = loss(val_output, val_target.float())
        
        loss_train.backward()
        optimizer.step()
        if e == 1 or e % 10 == 0:
            print(f'Epoch {e}: \ttrain loss {loss_train.item():.2f}\t validation loss {loss_val.item():.2f}')
``` -->

<p>E o loop de treino:</p>

<script src="https://gist.github.com/tutss/2d70a406f2b65c99044df099a92817c1.js"></script>

<!-- ```python
train(model, loss, optimizer, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, epochs=150)
``` -->

<h4 id="resultados-para-o-1-modelo">Resultados para o 1° modelo</h4>

<p>Podemos notar que o 1° modelo não contém uma função de ativação, e portanto, tem problemas em conseguir caracterizar funções mais complexas (não lineares). Um exercício interessante para analisar esse caso, é verificar a superfície de decisão:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/model1_pred.png" alt="model1pred" /></th>
      <th style="text-align: center"> </th>
      <th><img src="/files/images/model1_bound.png" alt="model1bound" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Predições feitas pelo 1° modelo</td>
      <td style="text-align: center"> </td>
      <td>Superfície de decisão do 1° modelo</td>
    </tr>
  </tbody>
</table>

<p>Podemos tentar expandir essa rede (ainda sem funções de ativação) para verificar o impacto do aumento de camadas:</p>

<script src="https://gist.github.com/tutss/f50499daa9dbf400fba31b1fa8c23943.js"></script>

<!-- ```python
model = nn.Sequential(
    nn.Linear(input_shape, 128),
    nn.Linear(128, 64),
    nn.Linear(64, 32),
    nn.Linear(32, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(model.parameters())
loss = nn.BCELoss()
``` -->

<p>Obtemos os seguintes resultados:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/model11_pred.png" alt="model11pred" /></th>
      <th style="text-align: center"> </th>
      <th><img src="/files/images/model11_bound.png" alt="model11bound" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Predições feitas pelo 1° modelo com um maior número de camadas.</td>
      <td style="text-align: center"> </td>
      <td>Superfície de decisão do 1° modelo com um maior número de camadas.</td>
    </tr>
  </tbody>
</table>

<p>Analisando as imagens acima, fica evidente que o melhor que a rede consegue é uma reta que separa, mesmo que não perfeitamente, as duas classes (vermelha e azul), coincidindo com o que esperávamos. Pelo fato de estarmos utilizando combinações lineares, padrões não lineares são quase imperceptíveis.</p>

<h3 id="2-modelo">2° modelo</h3>

<p>Agora, iremos introduzir uma função de ativação no modelo, e analisar o resultado obtido. A função de ativação que iremos selecionar é a função ReLU (<a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"><code class="language-plaintext highlighter-rouge">nn.ReLU</code></a>), que matematicamente é expressa como:</p>

\[f(x) = max(x, 0)\]

<p>A função ReLU, por definição, é não linear (pela aplicação do máximo), e é comumente utilizada no contexto de redes neurais. Introduzindo a função de ativação, a rede terá maior capacidade de captar comportamentos não lineares nos dados.</p>

<p>Assim, o modelo fica:</p>

<script src="https://gist.github.com/tutss/5f947c271a90144963f4d9aefdbcb476.js"></script>

<!-- ```python
non_linear_model = nn.Sequential(
    nn.Linear(input_shape, 128),
    nn.ReLU(),
    nn.Linear(128, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(non_linear_model.parameters())
loss = nn.BCELoss()
``` -->

<p>Treinando o modelo como na etapa anterior, temos a seguinte superfície de decisão:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/relu1.png" alt="relu1pred" /></th>
      <th style="text-align: center"> </th>
      <th><img src="/files/images/relu1_bound.png" alt="relu1bound" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Predições feitas pelo 2° modelo.</td>
      <td style="text-align: center"> </td>
      <td>Superfície de decisão do 2° modelo.</td>
    </tr>
  </tbody>
</table>

<p>Podemos perceber que agora, temos uma superfície mais curvilínea, devido a introdução de não linearidade pela camada <code class="language-plaintext highlighter-rouge">nn.ReLU</code>. O resultado seria semelhante caso utilizássemos outra função de ativação (não necessariamente ser igual).</p>

<h3 id="construindo-mais-modelos">Construindo mais modelos</h3>

<p>Para avaliar como as funções de ativação, atreladas a um maior número de camadas, permitem aproximarmos funções não lineares, iremos construir mais alguns modelos:</p>

<script src="https://gist.github.com/tutss/7caf231958a5ac53c07c2063e58cc0d8.js"></script>

<!-- ```python
non_linear_model = nn.Sequential(
    nn.Linear(input_shape, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(non_linear_model.parameters())
loss = nn.BCELoss()
``` -->

<script src="https://gist.github.com/tutss/a431a5e01f467632d79e2502927c22bd.js"></script>

<!-- ```python
non_linear_model = nn.Sequential(
    nn.Linear(input_shape, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(non_linear_model.parameters())
loss = nn.BCELoss()
``` -->

<p>Analisando a evolução da superfície de decisão:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/relu2_bound.png" alt="relu2bound" /></th>
      <th style="text-align: center"> </th>
      <th><img src="/files/images/relu3_bound.png" alt="relu3bound" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Predições feitas pelo 3° modelo.</td>
      <td style="text-align: center"> </td>
      <td>Superfície de decisão do 4° modelo.</td>
    </tr>
  </tbody>
</table>

<p>Fica evidente que construindo uma rede com mais camadas, atreladas a funções de ativação (não lineares), a nossa superfície de decisão consegue captar mais nuances presentes nos dados, permitindo uma classificação mais acurada. Claro, a ideia aqui é avaliar a superfície em si, já que em um contexto de uma aplicação real seria necessário avaliar também a questão da generalização da rede, que seria o quão bem a rede consegue classificar dados os quais nunca viu.</p>

<p>Verificamos que introduzindo as funções de ativação, junto ao aumento do número de camadas, nossa rede conseguiu captar as nuances da distribuição não linear dos dados.</p>

<hr />

<p>Nesse post, conseguimos analisar brevemente um dos principais impactos de funções de ativação dentro do contexto de redes neurais. O tópico é uma grande área de pesquisa, com constantes novas descobertas, e apenas pincelamos o seu uso dentro do cenário de classificação.</p>

<p><strong>O post também é acompanhado de um notebook, <a href="https://github.com/tutss/datascience/blob/master/nonlinearity.ipynb">neste link</a>.</strong></p>

<p>As referências para a construção desse material foram:</p>

<ul>
  <li><a href="https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf">Deep Learning with Pytorch</a></li>
  <li><a href="https://www.machinecurve.com/index.php/2020/10/29/why-nonlinear-activation-functions-improve-ml-performance-with-tensorflow-example/">Why nonlinear activation functions improve ML performance</a></li>
  <li><a href="https://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression?noredirect=1&amp;lq=1">Plotting decision boundary of logistic regression</a></li>
</ul>

<p>Até o próximo post :)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Dentro do contexto de redes neurais, as funções de ativação são essenciais. Uma de suas principais funções é permitir o aprendizado de regiões e superfícies de decisão mais complexas. Nesse post, iremos criar redes neurais usando Pytorch e analisar as superfícies de decisão com e sem funções de ativação.]]></summary></entry><entry><title type="html">Tutorial de K-Means</title><link href="http://localhost:4000/2020/09/11/k-means.html" rel="alternate" type="text/html" title="Tutorial de K-Means" /><published>2020-09-11T18:30:54-03:00</published><updated>2020-09-11T18:30:54-03:00</updated><id>http://localhost:4000/2020/09/11/k-means</id><content type="html" xml:base="http://localhost:4000/2020/09/11/k-means.html"><![CDATA[<h3 id="publiquei-um-novo-notebook-no-kaggle-falando-sobre-algoritmos-de-clustering-especialmente-o-k-means">Publiquei um novo notebook no Kaggle falando sobre algoritmos de clustering, especialmente o K-Means.</h3>

<p>Ao longo do notebook, são abordados:</p>

<h4 id="1-o-que-é-k-means-para-que-serve-e-como-podemos-utilizá-lo">1. O que é K-Means, para que serve e como podemos utilizá-lo.</h4>
<ul>
  <li>Como funciona e o que é.</li>
</ul>

<h4 id="2-problemas-relacionados-e-métricas-de-comparação">2. Problemas relacionados e métricas de comparação.</h4>
<ul>
  <li>Como avaliar um modelo no cenário não supervisionado?</li>
</ul>

<h4 id="3-spectral-clustering">3. Spectral Clustering</h4>
<ul>
  <li>Comparar o K-Means com outro algoritmo de clustering.</li>
</ul>

<h4 id="4-k-means-aplicado-ao-load_digits-subconjunto-do-mnist">4. K-Means aplicado ao <em>load_digits()</em> (subconjunto do MNIST)</h4>
<ul>
  <li>Aplicar o K-Means no reconhecimento de dígitos e comparar com outros algoritmos de clustering</li>
</ul>

<hr />

<p>A ideia central do algoritmo K-Means é <strong>clusterização</strong>, ou seja, dado um dataset, conseguir <strong>separar os dados em clusters</strong>, de forma a diferenciá-los com labels diferentes. É um algoritmo não supervisionado, pois não necessita de um valor real ou classificação par construir os clusters.</p>

<p>Por exemplo, para um conjunto de dados distribuído como:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/kmeans_1.png" alt="kmeans-1" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Dataset obtidos da função <em>make_blobs()</em>, da biblioteca <em>scikit-learn</em></td>
    </tr>
  </tbody>
</table>

<p>E aplicando o K-Means, chegamos na seguinte configuração de <em>labels</em>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/kmeans_2.png" alt="kmeans-2" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Dataset com os labels obtidos pelo K-Means</td>
    </tr>
  </tbody>
</table>

<p>O algoritmo do K-Means é relativamente simples. Dado um número k de clusters inicial (veremos porque isso pode ser um problema) selecionados aleatoriamente, são calculados as distâncias de cada ponto aos k pontos (centros) escolhidos e o ponto recebe a label do cluster mais próximo, re-calculamos os centros com base na média do cluster, até que os clusters não mudem.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/kmeans_3.png" alt="kmeans-3" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Aplicação do K-Means com k=3 e k=4, em um dataset obtido de uma distribuição normal multivariada</td>
    </tr>
  </tbody>
</table>

<p><strong>O notebook completo pode ser encontrado <a href="https://www.kaggle.com/arturmrs/tutorial-k-means">aqui!</a></strong></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Publiquei um novo notebook no Kaggle falando sobre algoritmos de clustering, especialmente o K-Means.]]></summary></entry></feed>