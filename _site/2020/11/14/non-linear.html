<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Não linearidade em redes neurais com Pytorch | tuts blog</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Não linearidade em redes neurais com Pytorch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Dentro do contexto de redes neurais, as funções de ativação são essenciais. Uma de suas principais funções é permitir o aprendizado de regiões e superfícies de decisão mais complexas. Nesse post, iremos criar redes neurais usando Pytorch e analisar as superfícies de decisão com e sem funções de ativação." />
<meta property="og:description" content="Dentro do contexto de redes neurais, as funções de ativação são essenciais. Uma de suas principais funções é permitir o aprendizado de regiões e superfícies de decisão mais complexas. Nesse post, iremos criar redes neurais usando Pytorch e analisar as superfícies de decisão com e sem funções de ativação." />
<link rel="canonical" href="http://localhost:4000/2020/11/14/non-linear.html" />
<meta property="og:url" content="http://localhost:4000/2020/11/14/non-linear.html" />
<meta property="og:site_name" content="tuts blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-14T15:16:54-03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Não linearidade em redes neurais com Pytorch" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-11-14T15:16:54-03:00","datePublished":"2020-11-14T15:16:54-03:00","description":"Dentro do contexto de redes neurais, as funções de ativação são essenciais. Uma de suas principais funções é permitir o aprendizado de regiões e superfícies de decisão mais complexas. Nesse post, iremos criar redes neurais usando Pytorch e analisar as superfícies de decisão com e sem funções de ativação.","headline":"Não linearidade em redes neurais com Pytorch","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/11/14/non-linear.html"},"url":"http://localhost:4000/2020/11/14/non-linear.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="tuts blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">tuts blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Não linearidade em redes neurais com Pytorch</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-11-14T15:16:54-03:00" itemprop="datePublished">Nov 14, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Dentro do contexto de redes neurais, as funções de ativação são essenciais. Uma de suas principais funções é permitir o aprendizado de regiões e superfícies de decisão mais complexas. Nesse post, iremos criar redes neurais usando <code class="language-plaintext highlighter-rouge">Pytorch</code> e analisar as superfícies de decisão com e sem funções de ativação.</p>

<h2 id="redes-neurais">Redes neurais</h2>

<p>Uma rede neural é um aproximador universal de funções. O que isso quer dizer? Basicamente, uma rede neural consegue aproximar qualquer função contínua (<a href="http://neuralnetworksanddeeplearning.com/chap4.html">dadas algumas condições</a>). Quando estamos tratando de aprendizado de máquina, de modo geral, a ideia é essencialmente aproximar uma função que descreve/aprende seus dados de treino.</p>

<p>Dessa forma, redes neurais são ferramentas muito poderosas em problemas de classificação, em que procuramos uma superfície de decisão para conseguir dizer a qual classe um objeto pertence, e regressão, em que predizemos um valor numérico. Nesse post, iremos abordar o uso das redes em problemas de classificação.</p>

<p>Algumas peças importantes compõem a arquitetura de uma rede neural, e os componentes básicos são: os neurônios (unidades básicas), a entrada (<em>inputs</em>), as camadas internas (<em>hidden layers</em>), as funções de ativação (<em>activation functions</em>) e a camada de saída (<em>outputs</em>). Na imagem abaixo, temos uma ilustração de uma rede:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/simple_nn.png" alt="neuralnet" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Rede neural com uma camada de entrada, uma camada interna</td>
    </tr>
    <tr>
      <td style="text-align: center">e uma camada de saída. Fonte: <a href="https://commons.wikimedia.org/wiki/File:Neuralnetwork.png">Wikipedia</a></td>
    </tr>
  </tbody>
</table>

<p>Não entraremos em detalhes sobre outros pontos igualmente relevantes no estudo de redes neurais, como funções de perda (<em>loss functions</em>), otimizadores (<em>optimizers</em>), <em>backpropagation</em>, etc. Em especial, iremos analisar o impacto das funções de ativação quando temos um problema de classificação com uma superfície não linear.</p>

<h3 id="exemplo-em-pytorch">Exemplo em Pytorch</h3>

<p>Um neurônio, unidade fundamental de uma rede neural, computa uma função linear do que recebe de entrada:
\(y = w*x + b\)</p>

<p>Como podemos perceber, essa equação define uma reta. Em <code class="language-plaintext highlighter-rouge">Pytorch</code>, podemos descrever essa função como:</p>

<script src="https://gist.github.com/tutss/eea6c62bd60a5d146613b982e51ab50d.js"></script>

<!-- ```python
import torch.nn as nn
model = nn.Linear(1,1)
y = model(x)
``` -->

<p>Se os neurônios definem retas, como uma rede pode aproximar tão bem funções complexas? Afinal, combinações de funções lineares são lineares. É ai que entra a importância das funções de ativação.</p>

<h2 id="funções-de-ativação">Funções de ativação</h2>

<p>Como comentado anteriormente, um dos principais papéis das funções de ativação é adicionar não linearidade na saída de um neurônio, permitindo que a rede consiga aprender superfícies de decisão mais complexas. Existem diferentes tipos de funções de ativação, veja na figura abaixo:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/activ_func.png" alt="activationfunc" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Exemplo de duas funções de ativação: ReLU e <em>Softplus</em>. Fonte: <a href="https://commons.wikimedia.org/wiki/File:Rectified_linear_and_Softplus_activation_functions.png">Wikipedia</a></td>
    </tr>
  </tbody>
</table>

<p>Um exemplo comum de função de ativação é a função sigmóide, descrita matematicamente como:
\(f(x) = \frac{1}{1+e^{-x}}\)
A função sigmóide é não linear e o seu resultado são valores no intervalo entre 0 e 1. Ela, como as demais funções de ativação, introduzem o fator não linearidade na rede quando um neurônio computa:
\(f(y) = f(w*x + b) = \frac{1}{1+e^{-(wx+b)}}\)</p>

<p>No nosso caso, o problema de classificação em questão é obtido pela função <code class="language-plaintext highlighter-rouge">make_moons</code> do pacote <code class="language-plaintext highlighter-rouge">scikit-learn</code>. Nele, temos duas features que descrevem duas classes, como na figura abaixo.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/makemoons.png" alt="makemoons" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Dataset <em>make_moons</em>.</td>
    </tr>
  </tbody>
</table>

<p>Iremos inicialmente montar uma pequena rede, sem nenhuma função de ativação, e verificar a superfície de decisão que ela obtêm.</p>

<h2 id="avaliando-as-redes-no-conjunto-make_moons">Avaliando as redes no conjunto <em>make_moons</em></h2>

<p>O pacote <a href="pytorch.org/">Pytorch</a> possui alguns módulos que auxiliam e simplificam a construção de uma rede neural. No nosso caso, iremos utilizar o módulo <code class="language-plaintext highlighter-rouge">nn</code>, que contém métodos e classes necessárias para a construção da rede, e o módulo <code class="language-plaintext highlighter-rouge">optim</code>, que contém os otimizadores para as redes.</p>

<p>Primeiro, é importante lembrar que o Pytorch trabalha com uma estrutura de dados chamada <strong>tensor</strong>. Um tensor seria uma generalização de um vetor em $N$ dimensões. Por exemplo, um vetor é um tensor com 1 dimensão e uma matriz é um tensor com 2 dimensões. O pacote <code class="language-plaintext highlighter-rouge">scikit-learn</code>, que contém a função para construirmos o conjunto de dados, utiliza vetores do <code class="language-plaintext highlighter-rouge">numpy</code>, e portanto, precisamos convertê-los para a estrutura <code class="language-plaintext highlighter-rouge">torch.tensor</code>.</p>

<h3 id="convertendo-para-tensor">Convertendo para tensor</h3>

<p>Obtemos o conjunto de dados:</p>

<script src="https://gist.github.com/tutss/45d7acb84bac1dde4b7fa7e5ed4321b4.js"></script>

<!-- ```python
n_samples = 1_000
X, y = make_moons(n_samples=n_samples, noise=0.1, random_state=42)
``` -->

<p>Dividimos o conjunto em treino e teste:</p>

<script src="https://gist.github.com/tutss/0c8299a296ac50a84b6d564921066e39.js"></script>

<!-- ```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)
``` -->

<p>Convertendo para tensor:</p>

<script src="https://gist.github.com/tutss/b4fbff0722d824d422bfdb9516883c2d.js"></script>

<!-- ```python
X_train_tensor = torch.from_numpy(X_train)
y_train_tensor = torch.from_numpy(y_train).unsqueeze(1)

<!-- X_val_tensor = torch.from_numpy(X_val)
y_val_tensor = torch.from_numpy(y_val).unsqueeze(1)
``` -->

<h3 id="módulos-nn-e-optim">Módulos <code class="language-plaintext highlighter-rouge">nn</code> e <code class="language-plaintext highlighter-rouge">optim</code></h3>

<p>O módulo <code class="language-plaintext highlighter-rouge">nn</code> contém componentes importantes na construção de uma rede, como o tipo das camadas, as funções de perda e ativação que iremos utilizar. Já o módulo <code class="language-plaintext highlighter-rouge">optim</code> contém os otimizadores para a rede.</p>

<script src="https://gist.github.com/tutss/3be84f7f761d67ec251d8465c86a4b46.js"></script>

<!-- ```python
import torch.nn as nn
import torch.optim as optim
``` -->

<hr />

<h2 id="construindo-as-redes">Construindo as redes</h2>

<p>Agora temos as peças necessárias para construir as redes que irão classificar o nosso conjunto. Assim como no pacote Keras, o Pytorch possui um módulo chamado <a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"><code class="language-plaintext highlighter-rouge">nn.Sequential</code></a> que facilita bastante a construção da rede. Também, precisamos de um otimizador, no nosso caso, <a href="https://pytorch.org/docs/stable/_modules/torch/optim/adagrad.html#Adagrad"><code class="language-plaintext highlighter-rouge">Adagrad</code></a> e a função de perda <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss"><code class="language-plaintext highlighter-rouge">nn.BCELoss</code></a>.</p>

<p>Aqui vale ressaltar que a escolha do otimizador e da função de perda não tiveram uma grande motivação, apenas que o <code class="language-plaintext highlighter-rouge">Adagrad</code> é um otimizador comum, e a função de perda é apropriada para classificação binária (<a href="https://en.wikipedia.org/wiki/Cross_entropy"><em>binary cross entropy</em></a>).</p>

<h3 id="1-modelo">1° modelo</h3>

<p>Para o primeiro modelo, com 100 unidades em uma única camada interna, temos:</p>

<script src="https://gist.github.com/tutss/744c13a2092b093b7b7dd948abcb16b4.js"></script>

<!-- ```python
model = nn.Sequential(
    nn.Linear(input_shape, 100),
    nn.Linear(100, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(model.parameters())
loss = nn.BCELoss()
``` -->

<p>Com a rede construída, precisamos treiná-la para que possa aprender as características dos dados e realizar as previsões. A função de treino:</p>

<script src="https://gist.github.com/tutss/6e55bb5f4ea271db57dc36da338aa262.js"></script>

<!-- ```python
def train(model, loss, optimizer, train_values, train_target, val_values, val_target, epochs=50):
    for e in range(1, epochs+1):
        optimizer.zero_grad()   
        output = model(train_values.float())
        loss_train = loss(output, train_target.float())
        
<!-- val_output = model(val_values.float())
        loss_val = loss(val_output, val_target.float())
        
        loss_train.backward()
        optimizer.step()
        if e == 1 or e % 10 == 0:
            print(f'Epoch {e}: \ttrain loss {loss_train.item():.2f}\t validation loss {loss_val.item():.2f}')
``` -->

<p>E o loop de treino:</p>

<script src="https://gist.github.com/tutss/2d70a406f2b65c99044df099a92817c1.js"></script>

<!-- ```python
train(model, loss, optimizer, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, epochs=150)
``` -->

<h4 id="resultados-para-o-1-modelo">Resultados para o 1° modelo</h4>

<p>Podemos notar que o 1° modelo não contém uma função de ativação, e portanto, tem problemas em conseguir caracterizar funções mais complexas (não lineares). Um exercício interessante para analisar esse caso, é verificar a superfície de decisão:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/model1_pred.png" alt="model1pred" /></th>
      <th style="text-align: center"> </th>
      <th><img src="/files/images/model1_bound.png" alt="model1bound" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Predições feitas pelo 1° modelo</td>
      <td style="text-align: center"> </td>
      <td>Superfície de decisão do 1° modelo</td>
    </tr>
  </tbody>
</table>

<p>Podemos tentar expandir essa rede (ainda sem funções de ativação) para verificar o impacto do aumento de camadas:</p>

<script src="https://gist.github.com/tutss/f50499daa9dbf400fba31b1fa8c23943.js"></script>

<!-- ```python
model = nn.Sequential(
    nn.Linear(input_shape, 128),
    nn.Linear(128, 64),
    nn.Linear(64, 32),
    nn.Linear(32, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(model.parameters())
loss = nn.BCELoss()
``` -->

<p>Obtemos os seguintes resultados:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/model11_pred.png" alt="model11pred" /></th>
      <th style="text-align: center"> </th>
      <th><img src="/files/images/model11_bound.png" alt="model11bound" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Predições feitas pelo 1° modelo com um maior número de camadas.</td>
      <td style="text-align: center"> </td>
      <td>Superfície de decisão do 1° modelo com um maior número de camadas.</td>
    </tr>
  </tbody>
</table>

<p>Analisando as imagens acima, fica evidente que o melhor que a rede consegue é uma reta que separa, mesmo que não perfeitamente, as duas classes (vermelha e azul), coincidindo com o que esperávamos. Pelo fato de estarmos utilizando combinações lineares, padrões não lineares são quase imperceptíveis.</p>

<h3 id="2-modelo">2° modelo</h3>

<p>Agora, iremos introduzir uma função de ativação no modelo, e analisar o resultado obtido. A função de ativação que iremos selecionar é a função ReLU (<a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"><code class="language-plaintext highlighter-rouge">nn.ReLU</code></a>), que matematicamente é expressa como:</p>

\[f(x) = max(x, 0)\]

<p>A função ReLU, por definição, é não linear (pela aplicação do máximo), e é comumente utilizada no contexto de redes neurais. Introduzindo a função de ativação, a rede terá maior capacidade de captar comportamentos não lineares nos dados.</p>

<p>Assim, o modelo fica:</p>

<script src="https://gist.github.com/tutss/5f947c271a90144963f4d9aefdbcb476.js"></script>

<!-- ```python
non_linear_model = nn.Sequential(
    nn.Linear(input_shape, 128),
    nn.ReLU(),
    nn.Linear(128, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(non_linear_model.parameters())
loss = nn.BCELoss()
``` -->

<p>Treinando o modelo como na etapa anterior, temos a seguinte superfície de decisão:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/relu1.png" alt="relu1pred" /></th>
      <th style="text-align: center"> </th>
      <th><img src="/files/images/relu1_bound.png" alt="relu1bound" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Predições feitas pelo 2° modelo.</td>
      <td style="text-align: center"> </td>
      <td>Superfície de decisão do 2° modelo.</td>
    </tr>
  </tbody>
</table>

<p>Podemos perceber que agora, temos uma superfície mais curvilínea, devido a introdução de não linearidade pela camada <code class="language-plaintext highlighter-rouge">nn.ReLU</code>. O resultado seria semelhante caso utilizássemos outra função de ativação (não necessariamente ser igual).</p>

<h3 id="construindo-mais-modelos">Construindo mais modelos</h3>

<p>Para avaliar como as funções de ativação, atreladas a um maior número de camadas, permitem aproximarmos funções não lineares, iremos construir mais alguns modelos:</p>

<script src="https://gist.github.com/tutss/7caf231958a5ac53c07c2063e58cc0d8.js"></script>

<!-- ```python
non_linear_model = nn.Sequential(
    nn.Linear(input_shape, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(non_linear_model.parameters())
loss = nn.BCELoss()
``` -->

<script src="https://gist.github.com/tutss/a431a5e01f467632d79e2502927c22bd.js"></script>

<!-- ```python
non_linear_model = nn.Sequential(
    nn.Linear(input_shape, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 1),
    nn.Sigmoid()
)
optimizer = optim.Adagrad(non_linear_model.parameters())
loss = nn.BCELoss()
``` -->

<p>Analisando a evolução da superfície de decisão:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/files/images/relu2_bound.png" alt="relu2bound" /></th>
      <th style="text-align: center"> </th>
      <th><img src="/files/images/relu3_bound.png" alt="relu3bound" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Predições feitas pelo 3° modelo.</td>
      <td style="text-align: center"> </td>
      <td>Superfície de decisão do 4° modelo.</td>
    </tr>
  </tbody>
</table>

<p>Fica evidente que construindo uma rede com mais camadas, atreladas a funções de ativação (não lineares), a nossa superfície de decisão consegue captar mais nuances presentes nos dados, permitindo uma classificação mais acurada. Claro, a ideia aqui é avaliar a superfície em si, já que em um contexto de uma aplicação real seria necessário avaliar também a questão da generalização da rede, que seria o quão bem a rede consegue classificar dados os quais nunca viu.</p>

<p>Verificamos que introduzindo as funções de ativação, junto ao aumento do número de camadas, nossa rede conseguiu captar as nuances da distribuição não linear dos dados.</p>

<hr />

<p>Nesse post, conseguimos analisar brevemente um dos principais impactos de funções de ativação dentro do contexto de redes neurais. O tópico é uma grande área de pesquisa, com constantes novas descobertas, e apenas pincelamos o seu uso dentro do cenário de classificação.</p>

<p><strong>O post também é acompanhado de um notebook, <a href="https://github.com/tutss/datascience/blob/master/nonlinearity.ipynb">neste link</a>.</strong></p>

<p>As referências para a construção desse material foram:</p>

<ul>
  <li><a href="https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf">Deep Learning with Pytorch</a></li>
  <li><a href="https://www.machinecurve.com/index.php/2020/10/29/why-nonlinear-activation-functions-improve-ml-performance-with-tensorflow-example/">Why nonlinear activation functions improve ML performance</a></li>
  <li><a href="https://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression?noredirect=1&amp;lq=1">Plotting decision boundary of logistic regression</a></li>
</ul>

<p>Até o próximo post :)</p>

  </div><a class="u-url" href="/2020/11/14/non-linear.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">tuts blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">tuts blog</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/tutss"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">tutss</span></a></li><li><a href="https://www.linkedin.com/in/arturmrsantos"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">arturmrsantos</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Hi, I&#39;m Artur! I&#39;m currently a Data Scientist at Amazon, working here in São Paulo. I usually don&#39;t post that much, only when I&#39;m feeling really inspired. Feel free to contact me if you have any questions.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
